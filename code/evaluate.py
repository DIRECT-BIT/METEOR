import os
import json
import argparse
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import openai
from tqdm import tqdm

def parse_arguments():
    parser = argparse.ArgumentParser(description="Evaluate LLMs using GPT-4 across multiple dimensions.")
    parser.add_argument("--model_paths", type=str, nargs='+', required=True, help="Paths to the LLMs to evaluate.")
    parser.add_argument("--test_data_path", type=str, required=True, help="Path to the test dataset.")
    parser.add_argument("--output_dir", type=str, required=True, help="Directory to save the evaluation results.")
    parser.add_argument("--api_key", type=str, default=None, help="Your OpenAI API key.")
    parser.add_argument("--max_retries", type=int, default=5, help="Maximum number of retries for API calls.")
    parser.add_argument("--batch_size", type=int, default=4, help="Batch size for model inference.")
    parser.add_argument("--max_eval_samples", type=int, default=None, help="Maximum number of test samples to evaluate.")
    return parser.parse_args()

def load_test_data(test_data_path, max_samples=None):
    data_list = []
    with open(test_data_path, 'r', encoding='utf-8') as f:
        for line in f:
            data = json.loads(line.strip())
            data_list.append(data)
            if max_samples and len(data_list) >= max_samples:
                break
    return data_list

def generate_responses(models, tokenizer, data_list, device, batch_size):
    model_outputs = {model_name: [] for model_name in models.keys()}
    for idx in tqdm(range(0, len(data_list), batch_size)):
        batch_data = data_list[idx:idx+batch_size]
        prompts = [f"Question: {item['question_content']}\nAnswer with reasoning:" for item in batch_data]

        inputs = tokenizer(prompts, return_tensors="pt", padding=True, truncation=True, max_length=512).to(device)

        for model_name, model in models.items():
            with torch.no_grad():
                outputs = model.generate(
                    input_ids=inputs["input_ids"],
                    attention_mask=inputs["attention_mask"],
                    max_length=1024,
                    do_sample=True,
                    temperature=0.7,
                    top_p=0.9,
                    eos_token_id=tokenizer.eos_token_id,
                    pad_token_id=tokenizer.pad_token_id,
                )
            batch_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)
            # Extract the answer by removing the prompt
            answers = [output.replace(prompt, "").strip() for output, prompt in zip(batch_outputs, prompts)]
            model_outputs[model_name].extend(answers)
    return model_outputs

def evaluate_responses_gpt4(question, answers, max_retries):
    evaluation_prompt = f"""You will be given a question and multiple answers generated by different models. Evaluate each answer based on the following criteria: Accuracy, Completeness, Relevance, Coherence, and Reliability. For each criterion, provide a score from 1 to 10 and a brief justification.

### Question:
{question}

### Answers:
"""
    for i, answer in enumerate(answers):
        evaluation_prompt += f"Answer {i+1}:\n{answer}\n\n"

    evaluation_prompt += "Please provide your evaluation in JSON format as a list, where each item corresponds to an answer and contains the scores and justifications for each criterion."

    for attempt in range(max_retries):
        try:
            response = openai.ChatCompletion.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": "You are an expert evaluator for educational content."},
                    {"role": "user", "content": evaluation_prompt}
                ],
                temperature=0,
                max_tokens=1500,
            )
            evaluation = response['choices'][0]['message']['content']
            return evaluation.strip()
        except openai.error.OpenAIError as e:
            print(f"Error: {e}")
            print(f"Retrying evaluation... ({attempt + 1}/{max_retries})")
    print("Maximum retries exceeded for evaluation.")
    return None

def main():
    args = parse_arguments()

    # Set OpenAI API key
    if args.api_key:
        openai.api_key = args.api_key
    else:
        openai.api_key = os.getenv("OPENAI_API_KEY")
        if not openai.api_key:
            raise ValueError("OpenAI API key must be provided via --api_key argument or OPENAI_API_KEY environment variable")

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Load the tokenizer
    print("Loading tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(args.model_paths[0], use_fast=False)

    # Load the models
    print("Loading models...")
    models = {}
    for model_path in args.model_paths:
        model_name = os.path.basename(model_path.rstrip('/'))
        print(f"Loading {model_name}...")
        model = AutoModelForCausalLM.from_pretrained(model_path).to(device)
        model.eval()
        models[model_name] = model

    # Load the test data
    print("Loading test data...")
    test_data = load_test_data(args.test_data_path, args.max_eval_samples)
    print(f"Total test samples: {len(test_data)}")

    # Generate responses from each model
    print("Generating responses from models...")
    model_responses = generate_responses(models, tokenizer, test_data, device, args.batch_size)

    # Prepare for evaluation
    output_dir = args.output_dir
    os.makedirs(output_dir, exist_ok=True)
    evaluation_results = []

    print("Evaluating responses using GPT-4...")
    for idx, item in enumerate(tqdm(test_data)):
        question = item['question_content']
        answers = []
        for model_name in models.keys():
            answers.append(model_responses[model_name][idx])
        evaluation = evaluate_responses_gpt4(question, answers, args.max_retries)
        if evaluation:
            result = {
                'question': question,
                'answers': {model_name: model_responses[model_name][idx] for model_name in models.keys()},
                'evaluation': evaluation
            }
            evaluation_results.append(result)
        else:
            print(f"Skipping evaluation for sample index {idx} due to API failure.")

    # Save evaluation results
    output_file = os.path.join(output_dir, "evaluation_results.jsonl")
    with open(output_file, 'w', encoding='utf-8') as f:
        for result in evaluation_results:
            json_line = json.dumps(result, ensure_ascii=False)
            f.write(json_line + '\n')
    print(f"Evaluation results saved to {output_file}")

if __name__ == "__main__":
    main()
